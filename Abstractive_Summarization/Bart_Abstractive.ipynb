{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bart_Abstractive.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LMHZOT5-l70_"},"source":["### LOADING DEPENDENCIES"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gaukKLlX2RtX","executionInfo":{"status":"ok","timestamp":1609719507936,"user_tz":300,"elapsed":3938,"user":{"displayName":"Zonair Nadeem","photoUrl":"","userId":"06819913743804129459"}},"outputId":"9904e1d4-0ad2-4018-f81a-c1a8f524a48f"},"source":["!pip install python-docx\n","import re\n","import glob\n","from docx import Document \n","import unicodedata\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","from spacy import displacy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: python-docx in /usr/local/lib/python3.6/dist-packages (0.8.10)\n","Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx) (4.2.6)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"paTFxsAmhVLl","executionInfo":{"status":"ok","timestamp":1609719510833,"user_tz":300,"elapsed":6825,"user":{"displayName":"Zonair Nadeem","photoUrl":"","userId":"06819913743804129459"}},"outputId":"543b6d07-6535-4442-9b3d-098018a11640"},"source":["# https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847/5\r\n","!pip install --no-cache-dir transformers sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LaBBW3xJmFQ9"},"source":["### LOADING DATA:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"loaw5bgLmw4_","executionInfo":{"status":"ok","timestamp":1609719510834,"user_tz":300,"elapsed":6818,"user":{"displayName":"Zonair Nadeem","photoUrl":"","userId":"06819913743804129459"}},"outputId":"7679179d-4fc7-4fa4-a8bd-93b599bf1ec5"},"source":["# We pushed the SOW documents on github and clone the github repo whenever we need the documents\n","#This is faster than mounting our colab notebook to google drive\n","!git clone https://github.com/NLP-Contracts/NLP-summarization.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fatal: destination path 'NLP-summarization' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tXMDQo4whgjS","executionInfo":{"status":"ok","timestamp":1609719510834,"user_tz":300,"elapsed":6812,"user":{"displayName":"Zonair Nadeem","photoUrl":"","userId":"06819913743804129459"}},"outputId":"49216704-ecea-4f46-f7ea-bfa55a60a70b"},"source":["%cd NLP-summarization/Sample\\ SoW\\ docs"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/NLP-summarization/Sample SoW docs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6iQNeMrUhkBc"},"source":["list_docsNames = glob.glob('*.docx') #USING GLOB TO GET NAMES OF PROJECTS\r\n"," \r\n","docs = []\r\n","st = \"\"\r\n","for docsName in list_docsNames:\r\n","  docs.append(st.join([p.text for p in Document(docsName).paragraphs])) #USING DOCUMENT TO LOAD PROJECTS"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y8JgpybIk3bY"},"source":["### DATA CLEANING:"]},{"cell_type":"code","metadata":{"id":"O2UNT09e2bL5"},"source":["#GENERIC CLEANING METHODS\n","# The basic_cleaner function will be applied first in the main corpus \n","#at which we will select the sections using regex pattern matching\n","def basic_cleaner(s):\n","  s = s.lower()\n","  s = re.sub(r'\\n', '', s)\n","  s = re.sub(r'\\t', '', s)\n","  s = re.sub(r' + ', '', s)\n","  return(s)\n","#after select each section using regex pattern matching\n","#we apply the extra_cleaner function to remove punctuations \n","#excpet \".\" & \",\", in addition the section numbers and information inside brackets \n","# will be removed as well\n","def extra_cleaner(s):\n","  # s= re.sub(',', ' ', s)\n","  s= re.sub('/', '', s)\n","  s= re.sub(':', '', s)\n","  s= re.sub('\\'', '', s)\n","  s= re.sub('-', '', s)\n","  s= re.sub('/', '', s)\n","  s= re.sub('<', '', s)\n","  s= re.sub('>', '', s)\n","  s=re.sub(r'\\d{1,2}\\.\\d', '', s) # remove subsection numbers by removing digit numbers (that have)\\d{1,2}\\.\\d pattern\n","  s=re.sub(r'\\([^)]*\\)', '', s) #remove brakets and anything thats inside the brakets\n","  return(s)\n","\n","################### need to implement capatization: \n","def sum_format_cleaner(sum): #To fix the output of each section. Captalizing and adding \".\" at the end of each section.\n","  return(' '.join(map(str,sum)))\n"," \n","c_docs = [basic_cleaner(_) for _ in docs]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OuZ71Il176Q3"},"source":["### Section Selection"]},{"cell_type":"code","metadata":{"id":"Gs_6tnJd7ciu"},"source":["# It has to be noted that often the section names are not consistant in all documents that is why we applied an \"or\", \"|\" symbol \r\n","# to find either or matched of a string.For instance the \"section9_charges\" section has \r\n","#\"charges, expenses and payment terms\" and in some documents\"fees, expenses and payment terms\" or \"expenses and payment terms\"\r\n","#which represnet the same section \r\n","section3_services=[' '.join(map(str, (re.findall('(?:.0services?|.sevices?| services 3.1?)(.*?) (?:term and schedule?|and schedule?|term and?)', i)))) for i in c_docs]\r\n","section4_schedule=[' '.join(map(str, (re.findall('(?:term and schedule?|and schedule?|term and?)(.*?)(?:place of performance?|place of performance and hours?|performance and hours)', i)))) for i in c_docs]\r\n","section5_PPH = [' '.join(map(str, (re.findall('(?:place of performance and hours?|performance and hours?)(.*?)(?:structure and roles|and roles?)', i)))) for i in c_docs]\r\n","section6_roles = [' '.join(map(str, (re.findall('(?:structure and roles ?|and roles?)(.*?)(?:general responsibilities|responsibilities?)', i)))) for i in c_docs]\r\n","section7_responsibilities = [' '.join(map(str, (re.findall('(?:general responsibilities)(.*?)(?:charges, expenses and payment terms|fees, expenses and payment terms?|expenses and payment terms?|milestones, deliverables, and acceptance criteria?|8.0 intentionally left blank?|.0 intentionally left blank)', i)))) for i in c_docs]\r\n","section9_charges = [' '.join(map(str, (re.findall('(?:charges, expenses and payment terms|fees, expenses and payment terms?|expenses and payment terms?)(.*?)(?:specific service levels)', i)))) for i in c_docs]\r\n","section12_assumptions=[' '.join(map(str, (re.findall('(?:assumptions and additional provisions?)(.*?)(?:addresses for administration and invoicing)', i)))) for i in c_docs]\r\n","section14_agreement = [' '.join(map(str, (re.findall('(?:.0 agreement?)(.*?)(?:agreed and accepted?)', i)))) for i in c_docs]\r\n","\r\n","#Applyinf the \"extra_cleaner\" function to take out brackets, subsection numbers and most of the punctuations\r\n","c_section3_services = [extra_cleaner(i) for i in section3_services]\r\n","c_section4_schedule = [extra_cleaner(i) for i in section4_schedule]\r\n","c_section5_PPH = [extra_cleaner(i) for i in section5_PPH]\r\n","c_section6_roles = [extra_cleaner(i) for i in section6_roles]\r\n","c_section7_responsibilities = [extra_cleaner(i) for i in section7_responsibilities]\r\n","c_section9_charges = [extra_cleaner(i) for i in section9_charges]\r\n","c_section12_assumptions = [extra_cleaner(i) for i in section12_assumptions]\r\n","c_section14_agreement = [extra_cleaner(i) for i in section14_agreement]\r\n","# This corpus is the collection of sections which we selectively chose to remain in the document\r\n","corpus = [] \r\n","for i in range(len(docs)): # range of the loop is the number of documents that are introduced \r\n","  corpus.append(c_section3_services[i] + \"\\n\"+ c_section4_schedule[i] + \"\\n\" + c_section5_PPH[i] + \"\\n\" + c_section6_roles[i] + \"\\n\" + c_section7_responsibilities[i] + \"\\n\" + c_section9_charges[i] + \"\\n\" + c_section12_assumptions[i] + \"\\n\" + c_section14_agreement[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"litDZ846k3KA"},"source":["# We are using Corpus[0]  \n","AKA--> TI_SOW_58_2019_TM_MITS_Stratus_mock.docx as our sample document to display our summarization results\n","It must be noted that we tested our model for all documents but for easy representation of our result we just decided to show one sample corpus to Mahmadul. "]},{"cell_type":"markdown","metadata":{"id":"MXApOURgxKKv"},"source":["###BART Model:\r\n","The Bart model was proposed in BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer on 29 Oct, 2019.\r\n","\r\n","According to the abstract:\r\n","\r\n","* Bart uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT).\r\n","* The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token.\r\n","* BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\r\n","\r\n","\r\n","https://huggingface.co/transformers/model_doc/bart.html\r\n","\r\n","https://github.com/pytorch/fairseq/tree/master/examples/bart\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"K2j-frDVp1vI"},"source":["from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig \r\n","\r\n","model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn') #Loading Bart facebook/bart-large-cnn pretrained model\r\n","tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6J-o2km0sy1_"},"source":["###FULL DOCUMENT SUMMARIZATION USING BART:"]},{"cell_type":"code","metadata":{"id":"h8m8jGXXsxtd"},"source":["def bart_full_summ(doc_num):\r\n","  #Full Corpus:\r\n","  ARTICLE_TO_SUMMARIZE = corpus[doc_num]\r\n","  inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt', truncation=True)\r\n","  # Generate Summary\r\n","  summary_ids = model.generate(inputs['input_ids'], num_beams=4, early_stopping=True)\r\n","  corpus_sum = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\r\n","  corpus_sum = sum_format_cleaner(corpus_sum)\r\n","  return(corpus_sum)\r\n","\r\n","\r\n","def summaize_full_sec_doc(n_doc):\r\n","  \r\n","  sum_full_sec= bart_full_summ(n_doc)\r\n","    \r\n","  if nlp(sum_full_sec).ents:\r\n","    displacy.render(nlp(sum_full_sec), style=\"ent\",jupyter=True) # shows the Named Entity Recognition labels as highlights if applicable to that summary\r\n","                                                        # This will assist the reader while looking at the summarized document\r\n","  else:\r\n","    display(sum_full_sec)                                 #If The section doesnt have NER labels then display the summarized section as is \r\n","  print(\"\\033[95m\" + \"Overal total words from the sectioned document After Summarization:\"+ \"\\033[0m\",(len(sum_full_sec.split())))\r\n","  print(\"\\033[95m\" + \"Overal total words from the sectioned document before Summarization:\"+ \"\\033[0m\",(len(corpus[n_doc].split())))\r\n","  print(\"\\033[95m\" + \"Ratio to the Original document: %\"+ \"\\033[0m\",(len(sum_full_sec.split())/(len(corpus[n_doc].split()))*100))\r\n","  print(\"\\033[95m\" + \"Overal Orignal document words before Summarization:\"+ \"\\033[0m\",(len(c_docs[n_doc].split())))\r\n","  print(\"\\033[95m\" + \"Ratio to the Original document: %\"+ \"\\033[0m\",(len(sum_full_sec.split())/(len(c_docs[n_doc].split()))*100))\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2zqSF-1CuLAO"},"source":["#Result/ Excecution of the bart_full_summ Summarization function"]},{"cell_type":"markdown","metadata":{"id":"O9oIMnSxlAdi"},"source":["This result shows that the Bart model doesnt retain valuable information such as starting and ending dates of the contract as well as the charges (CAD) related to the contract with only 57 words. That is why we approached this problem by summarizing the document section by section to retain the valuable information for whomever reading the summarized document."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":170},"id":"odiVLrDHuBtU","executionInfo":{"status":"ok","timestamp":1609719649923,"user_tz":300,"elapsed":33935,"user":{"displayName":"Zonair Nadeem","photoUrl":"","userId":"06819913743804129459"}},"outputId":"ad749398-38a0-4fd5-e13c-2e8242cf6772"},"source":["summaize_full_sec_doc(0)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">This sow provides a broad set of it services that are all delivered in a time and materials and staff augmentation delivery model. The resources provided by ti service representatives scope of duties are directed and managed by telus manager. The following activities and items are specifically excluded from the scope of services under this sow \n","<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    naterm\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n","</mark>\n",".</div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\u001b[95mOveral total words from the sectioned document After Summarization:\u001b[0m 57\n","\u001b[95mOveral total words from the sectioned document before Summarization:\u001b[0m 2452\n","\u001b[95mRatio to the Original document: %\u001b[0m 2.32463295269168\n","\u001b[95mOveral Orignal document words before Summarization:\u001b[0m 4246\n","\u001b[95mRatio to the Original document: %\u001b[0m 1.3424399434762129\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0HhxUtI_s4Go"},"source":["###SECTION BY SECTION SUMMARIZATION USING BART:"]},{"cell_type":"code","metadata":{"id":"-5ZJEcDjp12O"},"source":["def bart_section_sum(doc_num):     #Taking the Doc number that we want to summarize \r\n","          \r\n","  #Summarizing each section indivdually. \r\n","\r\n","  #Section3:\r\n","  ARTICLE_TO_SUMMARIZE = c_section3_services[doc_num]\r\n","  inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\r\n","  # Generate Summary\r\n","  summary_ids = model.generate(inputs['input_ids'], num_beams=4, early_stopping=True)\r\n","  service_sum = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\r\n","  service_sum = sum_format_cleaner(service_sum)\r\n","\r\n","\r\n","  #Section4:\r\n","  ARTICLE_TO_SUMMARIZE = c_section4_schedule[doc_num]\r\n","  inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\r\n","  # Generate Summary\r\n","  summary_ids = model.generate(inputs['input_ids'], num_beams=4, early_stopping=True)\r\n","  schedule_sum = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\r\n","  schedule_sum = sum_format_cleaner(schedule_sum)\r\n","\r\n","\r\n","  #Section5:\r\n","  ARTICLE_TO_SUMMARIZE = c_section5_PPH[doc_num]\r\n","  inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\r\n","  # Generate Summary\r\n","  summary_ids = model.generate(inputs['input_ids'], num_beams=4, early_stopping=True)\r\n","  PPH_sum = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\r\n","  PPH_sum = sum_format_cleaner(PPH_sum)\r\n","\r\n","\r\n","  #Section6:\r\n","  ARTICLE_TO_SUMMARIZE = c_section6_roles[doc_num]\r\n","  inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\r\n","  # Generate Summary\r\n","  summary_ids = model.generate(inputs['input_ids'], num_beams=4, early_stopping=True)\r\n","  role_sum = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\r\n","  role_sum = sum_format_cleaner(role_sum)\r\n","\r\n","  #Section7:\r\n","  ARTICLE_TO_SUMMARIZE = c_section7_responsibilities[doc_num]\r\n","  inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\r\n","  # Generate Summary\r\n","  summary_ids = model.generate(inputs['input_ids'], num_beams=4, early_stopping=True)\r\n","  resp_sum = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\r\n","  resp_sum = sum_format_cleaner(resp_sum)\r\n","\r\n","  #Section9:\r\n","  ARTICLE_TO_SUMMARIZE = c_section9_charges[doc_num]\r\n","  inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\r\n","  # Generate Summary\r\n","  summary_ids = model.generate(inputs['input_ids'], num_beams=4, early_stopping=True)\r\n","  charge_sum = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\r\n","  charge_sum = sum_format_cleaner(charge_sum)\r\n","\r\n","\r\n","  #Section12:\r\n","  ARTICLE_TO_SUMMARIZE = c_section12_assumptions[doc_num]\r\n","  inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\r\n","  # Generate Summary\r\n","  summary_ids = model.generate(inputs['input_ids'], num_beams=4, early_stopping=True)\r\n","  assum_sum = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\r\n","  assum_sum = sum_format_cleaner(assum_sum)\r\n","\r\n","  #Section14:\r\n","  ARTICLE_TO_SUMMARIZE = c_section14_agreement[doc_num]\r\n","  inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\r\n","  # Generate Summary\r\n","  summary_ids = model.generate(inputs['input_ids'], num_beams=4, early_stopping=True)\r\n","  agree_sum = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\r\n","  agree_sum = sum_format_cleaner(agree_sum)\r\n","\r\n","  return service_sum, schedule_sum, PPH_sum , role_sum, resp_sum, charge_sum, assum_sum, agree_sum\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vEHk2ELxqZ5h"},"source":["def summaize_by_section(n_doc):\r\n","  \r\n","  service_sum,schedule_sum,PPH_sum,role_sum,resp_sum, charge_sum,assum_sum,agree_sum= bart_section_sum(n_doc) #This function t5_section(n_doc) \r\n","                                                                                                        #will select the document number \r\n","                                                                                                        #which we want to summarize and spits out seven\r\n","                                                                                                        #summarized sections which will be displayed in the \r\n","                                                                                                        #for loop below\r\n","  original_sections=[c_section3_services,c_section4_schedule,c_section5_PPH,c_section6_roles,c_section7_responsibilities,c_section9_charges,c_section12_assumptions,c_section14_agreement]\r\n","  sum_sections={\"Services\":service_sum,\r\n","            \"Schedule\":schedule_sum,\r\n","            \"Place of Performance and Hours\":PPH_sum,\r\n","            \"Role\":role_sum,\r\n","            \"Responsibilities\":resp_sum,\r\n","            \"Charge\":charge_sum,\r\n","            \"Assumptions\":assum_sum,\r\n","            \"Agreement\":agree_sum}\r\n","\r\n","  for v,i in zip(sum_sections.items(),original_sections): #I zipped sum_sections and original_sections so to display \r\n","                                                          #the length of words in the original_sections before summarization and after summarization\r\n","\r\n","    print(\"\\033[95m\" + v[0]+ \"\\033[0m\") #sum_sections is a dictionary which in a zip will be transformed to a list of lists \r\n","                                        #where the list with index 0 becomes the keys and \r\n","                                        #list with index 1 becomes the summarized strings\r\n","                                        #This print will display the section names\r\n","    print(\"\\t\")\r\n","\r\n","\r\n","    if nlp(v[1]).ents:\r\n","     displacy.render(nlp(v[1]), style=\"ent\",jupyter=True) # shows the Named Entity Recognition labels as highlights if applicable to that summary\r\n","                                                          # This will assist the reader while looking at the summarized document\r\n","    else:\r\n","      display(v[1])                                       #If The section doesnt have NER labels then display the summarized section as is \r\n","    \r\n","    print(\"\\t\")\r\n","    print(\"\\033[33m\" + \"summarized lenght of section\"+ \"\\033[0m\",len(v[1].split())) # displays the word length of the sum_sections \r\n","    print(\"\\033[33m\" + \"original lenght of section\"+ \"\\033[0m\",len(i[n_doc].split())) # displays the word length of the original_sections \r\n","    \r\n","#this for loop below helps in calculating the overal word lenght of the full summarized document with all sections \r\n","  sum=0\r\n","  h=[]\r\n","  for k,v in sum_sections.items():\r\n","    h.append(len(v.split()))\r\n","  for i in h:\r\n","    sum=i+sum\r\n","  print(\"\\t\")\r\n","  print(\"\\033[95m\" + \"Overal total words from the sectioned document in the Summarized Version:\"+ \"\\033[0m\",sum)\r\n","  print(\"\\033[95m\" + \"Overal total words from the sectioned document before Summarization:\"+ \"\\033[0m\",(len(corpus[n_doc].split())))\r\n","  print(\"\\033[95m\" + \"Ratio to the sectioned document: %\"+ \"\\033[0m\",(sum/(len(corpus[n_doc].split())))*100)\r\n","  print(\"\\033[95m\" + \"Overal Orignal document words before Summarization:\"+ \"\\033[0m\",(len(c_docs[n_doc].split())))\r\n","  print(\"\\033[95m\" + \"Ratio to the Original document: %\"+ \"\\033[0m\",(sum/(len(c_docs[n_doc].split())))*100)\r\n","\r\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RakWbCChqZ9R","executionInfo":{"status":"ok","timestamp":1609720263457,"user_tz":300,"elapsed":205607,"user":{"displayName":"Zonair Nadeem","photoUrl":"","userId":"06819913743804129459"}},"outputId":"983af923-01e6-48ba-ec00-ff7f407d4e17"},"source":["summaize_by_section(0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[95mServices\u001b[0m\n","\t\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'subject to the agreement, the sowspecific scope of services shall include the following. this sow provides a broad set of it services that are all delivered in a time and materials and staff augmentation delivery model. the resources provided by ti service representatives are directed and managed by telus manager and their scope of duties is therefore open to change.'"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\t\n","\u001b[33msummarized lenght of section\u001b[0m 60\n","\u001b[33moriginal lenght of section\u001b[0m 122\n","\u001b[95mSchedule\u001b[0m\n","\t\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">this statement of work shall commence on \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    january, 01st, 2019\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n","  and shall end on \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    december\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n",", 31st,2019 , with the period from sow start date to sow end date referred to as , unless terminated earlier in accordance with the agreement.without limiting the terms of the agreement, any time during the sow term, telus may terminate this sow early for convenience by providing ti with a notice of \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    at least thirty\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n","  calendar days.</div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\t\n","\u001b[33msummarized lenght of section\u001b[0m 72\n","\u001b[33moriginal lenght of section\u001b[0m 147\n","\u001b[95mPlace of Performance and Hours\u001b[0m\n","\t\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">ti shall perform the services  at the following ti facilitiescanadian ti facilities none other \n","<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    north american\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n","</mark>\n"," ti facilities  none. offshore ti service representatives will perform services under this sow during various business and nonbusiness hours specific to each unique role. generally the work calendar will adhere to the telus \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    canada\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," working calendar however the detailed schedule for each role and approval for adhoc nonworking days shall be set.</div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\t\n","\u001b[33msummarized lenght of section\u001b[0m 68\n","\u001b[33moriginal lenght of section\u001b[0m 192\n","\u001b[95mRole\u001b[0m\n","\t\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'The parties shall appoint the following key personnel for the sow term. The ti manager will be responsible for the overall performance, delivery and management of services in respect of this sow. The telus manager shall manage and direct telus representatives and ti in accordance with this sow and the agreement.'"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\t\n","\u001b[33msummarized lenght of section\u001b[0m 51\n","\u001b[33moriginal lenght of section\u001b[0m 375\n","\u001b[95mResponsibilities\u001b[0m\n","\t\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'ti responsibilitiesti shall be responsible for the provision of all services in accordance with the service levels, if any, as attached to this sow per appendix ‘a’ , and as such, ti will retain overall project management responsibility for all ti service levels and ti service level impacting activities. ti will follow direction of the telus manager and other managers as from time to time designated by thetelus manager.'"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\t\n","\u001b[33msummarized lenght of section\u001b[0m 69\n","\u001b[33moriginal lenght of section\u001b[0m 383\n","\u001b[95mCharge\u001b[0m\n","\t\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">payment terms are set forth in the agreement, \n","<mark class=\"entity\" style=\"background: #ff8197; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    article 8.based on section 1\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LAW</span>\n","</mark>\n",", description, ti agrees to perform the services under this sow at the hourly rates quoted in \n","<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    canadian\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n","</mark>\n"," dollars in section .the maximum total estimated fees are as follows cad \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    700,36\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n",". overtime for each calendar month, ti may charge telus for the daily hours of ti representatives overtime \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    hours\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n","</mark>\n",".</div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\t\n","\u001b[33msummarized lenght of section\u001b[0m 61\n","\u001b[33moriginal lenght of section\u001b[0m 585\n","\u001b[95mAssumptions\u001b[0m\n","\t\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">ti acknowledges that it shall not store, transfer, transmit, view, access, disclose, process, handle, use or otherwise exploit, directly or indirectly, any restricted data outside of \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    canada\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n",". “restricted data” means all personal information;  confidential information of telus, as disclosing party, that relates to any telus customer.</div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\t\n","\u001b[33msummarized lenght of section\u001b[0m 46\n","\u001b[33moriginal lenght of section\u001b[0m 577\n","\u001b[95mAgreement\u001b[0m\n","\t\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'the parties acknowledge and agree that the terms and conditions of the agreement shall govern this statement of work. this sow and any change orders issued hereunder may be executed by the exchange of signed counterparts by facsimile transmission or electronically in pdf or similar secure format. when taken together will constitute one and the same document.'"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\t\n","\u001b[33msummarized lenght of section\u001b[0m 57\n","\u001b[33moriginal lenght of section\u001b[0m 71\n","\t\n","\u001b[95mOveral total words from the sectioned document in the Summarized Version:\u001b[0m 484\n","\u001b[95mOveral total words from the sectioned document before Summarization:\u001b[0m 2452\n","\u001b[95mRatio to the sectioned document: %\u001b[0m 19.73898858075041\n","\u001b[95mOveral Orignal document words before Summarization:\u001b[0m 4246\n","\u001b[95mRatio to the Original document: %\u001b[0m 11.398963730569948\n"],"name":"stdout"}]}]}